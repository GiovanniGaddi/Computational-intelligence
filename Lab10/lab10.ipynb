{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice, random\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of possible combinations of states\n",
    "# 2*9*(8*(7*(6*(5*(4*(3*(2*(1+1)+1)+1)+1)+1)+1)+1)+1)\n",
    "ALL_COMB = 2*(362880 + 362880 + 181440 + 60480 + 15120 + 3024 + 504 + 72 + 9)\n",
    "\n",
    "ITERATIONS = 500_000\n",
    "\n",
    "RANDOM_FIRST_TURN = False #Overwrites FIRST_TO_PLAY\n",
    "FIRST_TO_PLAY = False\n",
    "\n",
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated state_value based on order of turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(turn, pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins, +0,5 in case of draw if playing second\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        if turn:\n",
    "            return 0\n",
    "        else: \n",
    "            return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    x_turn = choice([True, False])\n",
    "    while available:\n",
    "        cell = choice(list(available))\n",
    "        if x_turn:\n",
    "            state.x.add(cell)\n",
    "        else:\n",
    "            state.o.add(cell)\n",
    "        x_turn =  not x_turn\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(cell)\n",
    "        if win(state.x) or win(state.o):\n",
    "            break\n",
    "    return x_turn, trajectory\n",
    "\n",
    "def learning_game(value_dict, update_rate=1, first_to_play = True):\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    x_turn = bool(first_to_play)\n",
    "    # exp based on chosen one but also on generated possible combination of moves, best case update_rate\n",
    "    exp_rate = len(value_dict)/ALL_COMB * update_rate\n",
    "    while available:\n",
    "        cell = choice(list(available))\n",
    "        if x_turn and exp_rate > random():\n",
    "            max_value = -ITERATIONS\n",
    "            for av_cell in available:\n",
    "                hashable_state = (frozenset(state.x.copy().union({av_cell})), frozenset(state.o))\n",
    "                #evaluate all possible moves\n",
    "                if max_value < value_dict[hashable_state]:\n",
    "                    max_value = value_dict[hashable_state]\n",
    "                    cell = av_cell\n",
    "        if x_turn:\n",
    "            state.x.add(cell)\n",
    "        else:\n",
    "            state.o.add(cell)\n",
    "        x_turn = not x_turn\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(cell)\n",
    "        if win(state.x) or win(state.o):\n",
    "            break\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the learning to the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game(value_dict, first_to_play = True):\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    x_turn = bool(first_to_play)\n",
    "    # exp based on chosen one but also on generated possible combination of moves, best case update_rate\n",
    "    while available:\n",
    "        cell = choice(list(available))\n",
    "        if x_turn:\n",
    "            max_value = -ITERATIONS\n",
    "            for av_cell in available:\n",
    "                hashable_state = (frozenset(state.x.copy().union({av_cell})), frozenset(state.o))\n",
    "                #evaluate all possible moves\n",
    "                if max_value < value_dict[hashable_state]:\n",
    "                    max_value = value_dict[hashable_state]\n",
    "                    cell = av_cell\n",
    "        if x_turn:\n",
    "            state.x.add(cell)\n",
    "        else:\n",
    "            state.o.add(cell)\n",
    "        x_turn = not x_turn\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(cell)\n",
    "        if win(state.x) or win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6455.12it/s]\n",
      "100%|██████████| 50000/50000 [00:07<00:00, 7063.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The player was not the First to Play\n",
      "Games_won: 87.69%\n",
      "Games_draw: 11.40%\n",
      "Games_lost: 0.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "first = FIRST_TO_PLAY\n",
    "#training\n",
    "for steps in tqdm(range(ITERATIONS)):\n",
    "    if RANDOM_FIRST_TURN: \n",
    "        first = choice([True, False])\n",
    "    trajectory = learning_game(value_dictionary, 100, first)\n",
    "    final_reward = state_value(first, trajectory[-1])\n",
    "    #print(\"Final reward: \", final_reward)\n",
    "    for state in trajectory:\n",
    "        #print(state, end=\": \")\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hit_state[hashable_state] += 1\n",
    "        #print(value_dictionary[hashable_state], end=\",\")\n",
    "        value_dictionary[hashable_state] = value_dictionary[hashable_state] + epsilon * (final_reward - value_dictionary[hashable_state])\n",
    "        #print(value_dictionary[hashable_state])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_count = 0\n",
    "draw_count = 0\n",
    "lose_count = 0\n",
    "for steps in tqdm(range(ITERATIONS)):\n",
    "    if RANDOM_FIRST_TURN: \n",
    "        first = choice([True, False])\n",
    "    trajectory = game(value_dictionary, first)\n",
    "    final_reward = state_value(first, trajectory[-1])\n",
    "    if final_reward > 0.5:\n",
    "        win_count += 1\n",
    "    elif final_reward < 0:\n",
    "        lose_count += 1\n",
    "    else :\n",
    "        draw_count += 1\n",
    "total_games = win_count+draw_count+ lose_count\n",
    "if not RANDOM_FIRST_TURN:\n",
    "    print(f\"The player {\"was\" if FIRST_TO_PLAY else \"was not\"} the First to Play\")\n",
    "else:\n",
    "    print(\"Turns were Randomized\")\n",
    "print(f\"Games_won: {win_count/total_games:.2%}\")\n",
    "print(f\"Games_draw: {draw_count/total_games:.2%}\")\n",
    "print(f\"Games_lost: {lose_count/total_games:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({5}), frozenset({7})), 0.36333923074662444),\n",
       " ((frozenset({5}), frozenset({9})), 0.3531043416953304),\n",
       " ((frozenset({5}), frozenset({4})), 0.26567752540382633),\n",
       " ((frozenset(), frozenset({7})), 0.24927147115187617),\n",
       " ((frozenset(), frozenset({9})), 0.24209722269717573),\n",
       " ((frozenset({5}), frozenset({8})), 0.24185137857390365),\n",
       " ((frozenset({5, 7, 8, 9}), frozenset({1, 2, 3, 4, 6})), 0.2340542374160329),\n",
       " ((frozenset({4}), frozenset({3})), 0.2309827057385861),\n",
       " ((frozenset({5}), frozenset({2})), 0.23097013226137186),\n",
       " ((frozenset({4, 5, 7, 8}), frozenset({1, 2, 3, 6, 9})), 0.23003295582574798)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter(lambda e: len(e.x)+ len(e.o) > 5 ,value_dictionary.items())\n",
    "sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5477"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hit_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-P-7LqQ3C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
